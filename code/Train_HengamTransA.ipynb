{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wYfViVSz2s-"
      },
      "source": [
        "# Train HengamTransA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JORUT1vz0SX"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q pytorch-lightning==1.5.10\n",
        "! pip install -q pytorch-crf==0.7.2\n",
        "! pip install -q transformers==4.16.2\n",
        "! pip install -q seqeval==1.2.2\n",
        "! pip install -q gdown --upgrade\n",
        "! pip install -q tqdm==4.62.3"
      ],
      "metadata": {
        "id": "j9WUHqsNzunc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l11ckQCD5JWs"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tMl9zr3l5JWt"
      },
      "outputs": [],
      "source": [
        "# import primitive libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# import seqval to report classifier performance metrics\n",
        "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "# import torch related modules\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "\n",
        "# import pytorch lightning library\n",
        "import pytorch_lightning as pl\n",
        "from torchcrf import CRF as SUPERCRF\n",
        "\n",
        "# import NLTK to create better tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Transformers : Roberta Model\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "from transformers import XLMRobertaModel, XLMRobertaConfig\n",
        "\n",
        "# import sklearn inorder to split data into train-evaluation-test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import Typings\n",
        "from typing import Union,Dict,List,Tuple,Any,Optional\n",
        "\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Data and Models"
      ],
      "metadata": {
        "id": "94cYmKoM5L6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model, gold, strong folers to store trained model and dataset.\n",
        "! mkdir model\n",
        "! mkdir gold\n",
        "! mkdir strong"
      ],
      "metadata": {
        "id": "EdaaxWXDOBZt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download strong data\n",
        "! wget https://huggingface.co/datasets/kargaranamir/HengamCorpus/raw/main/strong.txt -O ./strong/strong.txt\n",
        "\n",
        "# download previous HengamTransW\n",
        "! wget https://huggingface.co/kargaranamir/Hengam/resolve/main/HengamTransW.pth -O ./model/HengamTransW.pth\n",
        "\n",
        "# test gold data\n",
        "! wget https://huggingface.co/datasets/kargaranamir/HengamCorpus/raw/main/gold.txt -O ./gold/gold.txt"
      ],
      "metadata": {
        "id": "IKxNpHRGNQeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_std_ner_series(data):\n",
        "    output = []\n",
        "    # the loop notices the change of sentence with double newline \n",
        "    for sentence in filter(lambda x: len(x)>2, data.split('\\n\\n')):\n",
        "        sample = []\n",
        "        # each word laid in sepertaed lines\n",
        "        for wordline in sentence.split('\\n'):\n",
        "            if wordline=='':\n",
        "                continue\n",
        "            # the word and label are seperated from each other with tab \n",
        "            word, label = wordline.split('\\t')\n",
        "            sample.append((word, label))\n",
        "        if len(sample) > 255:\n",
        "            continue\n",
        "        output.append(sample)\n",
        "\n",
        "    # Convert data to Pandas Series\n",
        "    output = pd.Series(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "# A function to write train/val/test data in standard NER format\n",
        "def write_file(input_series, path):\n",
        "    with open(path, 'w') as file:\n",
        "        for row in input_series:\n",
        "            for token,tag in row:\n",
        "                file.write(token + '\\t' + tag + '\\n')\n",
        "            file.write('\\n')\n",
        "\n",
        "# Read Text file\n",
        "with open('strong/strong.txt','r') as f:\n",
        "    strong_data_text = f.read()\n",
        "\n",
        "strong_data = convert_text_to_std_ner_series(strong_data_text)\n",
        "\n",
        "# Split dataset with the following proportions:\n",
        "# Train: 7/8\n",
        "# Eval: 1/8\n",
        "strong_train, strong_val = train_test_split(strong_data, test_size=0.125, shuffle=True)\n",
        "\n",
        "# Write Files\n",
        "write_file(strong_train, 'strong/strong_train.txt')\n",
        "write_file(strong_val, 'strong/strong_val.txt')"
      ],
      "metadata": {
        "id": "-9qAMmo1zc1p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmK6V-wGzx7R"
      },
      "source": [
        "### Paths and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqaswDc55pGu"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"./model\" # Root directory of pytorch model\n",
        "TAGS_TABLE = ['B-TIM', 'I-TIM', 'B-DAT', 'I-DAT', 'O'] # Create Tag table\n",
        "MODEL_NAME = \"HengamTransA.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiFCEopXNmRp"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l35D2afgGuxg"
      },
      "outputs": [],
      "source": [
        "# import primitive libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# import seqval to report classifier performance metrics\n",
        "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "# import torch related modules\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "\n",
        "# import pytorch lightning library\n",
        "import pytorch_lightning as pl\n",
        "from torchcrf import CRF as SUPERCRF\n",
        "\n",
        "# import NLTK to create better tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Transformers : Roberta Model\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "from transformers import XLMRobertaModel, XLMRobertaConfig\n",
        "\n",
        "# import sklearn inorder to split data into train-evaluation-test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import Typings\n",
        "from typing import Union,Dict,List,Tuple,Any,Optional\n",
        "\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJujx4BAUcdY"
      },
      "outputs": [],
      "source": [
        "# for sent tokenizer (nltk)\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ASUZ2n2aR2"
      },
      "source": [
        "## XLM-Roberta "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs5yGEFN2gHP"
      },
      "source": [
        "### TokenFromSubtoken\n",
        "- Code adapted from the following [file](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py)\n",
        "- DeepPavlov is an popular open source library for deep learning end-to-end dialog systems and chatbots.\n",
        "- Licensed under the Apache License, Version 2.0 (the \"License\");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlP1uKJ9T8e4"
      },
      "outputs": [],
      "source": [
        "class TokenFromSubtoken(torch.nn.Module):\n",
        "\n",
        "    def forward(self, units: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" Assemble token level units from subtoken level units\n",
        "        Args:\n",
        "            units: torch.Tensor of shape [batch_size, SUBTOKEN_seq_length, n_features]\n",
        "            mask: mask of token beginnings. For example: for tokens\n",
        "                    [[``[CLS]`` ``My``, ``capybara``, ``[SEP]``],\n",
        "                    [``[CLS]`` ``Your``, ``aar``, ``##dvark``, ``is``, ``awesome``, ``[SEP]``]]\n",
        "                the mask will be\n",
        "                    [[0, 1, 1, 0, 0, 0, 0],\n",
        "                    [0, 1, 1, 0, 1, 1, 0]]\n",
        "        Returns:\n",
        "            word_level_units: Units assembled from ones in the mask. For the\n",
        "                example above this units will correspond to the following\n",
        "                    [[``My``, ``capybara``],\n",
        "                    [``Your`, ``aar``, ``is``, ``awesome``,]]\n",
        "                the shape of this tensor will be [batch_size, TOKEN_seq_length, n_features]\n",
        "        \"\"\"\n",
        "        \n",
        "        device = units.device\n",
        "        nf_int = units.size()[-1]\n",
        "        batch_size = units.size()[0]\n",
        "\n",
        "        # number of TOKENS in each sentence\n",
        "        token_seq_lengths = torch.sum(mask, 1).to(torch.int64)\n",
        "        # number of words\n",
        "        n_words = torch.sum(token_seq_lengths)\n",
        "        # max token seq len\n",
        "        max_token_seq_len = torch.max(token_seq_lengths)\n",
        "\n",
        "        idxs = torch.stack(torch.nonzero(mask, as_tuple=True), dim=1)\n",
        "        # padding is for computing change from one sample to another in the batch\n",
        "        sample_ids_in_batch = torch.nn.functional.pad(input=idxs[:, 0], pad=[1, 0])\n",
        "        \n",
        "        a = (~torch.eq(sample_ids_in_batch[1:], sample_ids_in_batch[:-1])).to(torch.int64)\n",
        "        \n",
        "        # transforming sample start masks to the sample starts themselves\n",
        "        q = a * torch.arange(n_words, device=device).to(torch.int64)\n",
        "        count_to_substract = torch.nn.functional.pad(torch.masked_select(q, q.to(torch.bool)), [1, 0])\n",
        "\n",
        "        new_word_indices = torch.arange(n_words, device=device).to(torch.int64) - count_to_substract[torch.cumsum(a, 0)]\n",
        "        \n",
        "        n_total_word_elements = max_token_seq_len*torch.ones_like(token_seq_lengths, device=device).sum()\n",
        "        word_indices_flat = (idxs[:, 0] * max_token_seq_len + new_word_indices).to(torch.int64)\n",
        "        #x_mask = torch.sum(torch.nn.functional.one_hot(word_indices_flat, n_total_word_elements), 0)\n",
        "        #x_mask = x_mask.to(torch.bool)\n",
        "        x_mask = torch.zeros(n_total_word_elements, dtype=torch.bool, device=device)\n",
        "        x_mask[word_indices_flat] = torch.ones_like(word_indices_flat, device=device, dtype=torch.bool)\n",
        "        # to get absolute indices we add max_token_seq_len:\n",
        "        # idxs[:, 0] * max_token_seq_len -> [0, 0, 0, 1, 1, 2] * 2 = [0, 0, 0, 3, 3, 6]\n",
        "        # word_indices_flat -> [0, 0, 0, 3, 3, 6] + [0, 1, 2, 0, 1, 0] = [0, 1, 2, 3, 4, 6]\n",
        "        # total number of words in the batch (including paddings)\n",
        "        # batch_size * max_token_seq_len -> 3 * 3 = 9\n",
        "        # tf.one_hot(...) ->\n",
        "        # [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "        #  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
        "        #  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
        "        #  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
        "        #  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
        "        #  [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
        "        #  x_mask -> [1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
        "        nonword_indices_flat = (~x_mask).nonzero().squeeze(-1)\n",
        "\n",
        "        # get a sequence of units corresponding to the start subtokens of the words\n",
        "        # size: [n_words, n_features]\n",
        "        \n",
        "        elements = units[mask.bool()]\n",
        "\n",
        "        # prepare zeros for paddings\n",
        "        # size: [batch_size * TOKEN_seq_length - n_words, n_features]\n",
        "        paddings = torch.zeros_like(nonword_indices_flat, dtype=elements.dtype).unsqueeze(-1).repeat(1,nf_int).to(device)\n",
        "\n",
        "        # tensor_flat -> [x, x, x, x, x, 0, x, 0, 0]\n",
        "        tensor_flat_unordered = torch.cat([elements, paddings])\n",
        "        _, order_idx = torch.sort(torch.cat([word_indices_flat, nonword_indices_flat]))\n",
        "        tensor_flat = tensor_flat_unordered[order_idx]\n",
        "\n",
        "        tensor = torch.reshape(tensor_flat, (-1, max_token_seq_len, nf_int))\n",
        "        # tensor -> [[x, x, x],\n",
        "        #            [x, x, 0],\n",
        "        #            [x, 0, 0]]\n",
        "\n",
        "        return tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2We9RK24f29"
      },
      "source": [
        "### Conditional Random Field \n",
        "- Code adopted form [torchcrf library](https://pytorch-crf.readthedocs.io/en/stable/)\n",
        "- we override veiterbi decoder in order to make it compatible with our code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjXultXaUj-t"
      },
      "outputs": [],
      "source": [
        "class CRF(SUPERCRF):\n",
        "\n",
        "    # override veiterbi decoder in order to make it compatible with our code \n",
        "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
        "                        mask: torch.ByteTensor) -> List[List[int]]:\n",
        "        # emissions: (seq_length, batch_size, num_tags)\n",
        "        # mask: (seq_length, batch_size)\n",
        "        assert emissions.dim() == 3 and mask.dim() == 2\n",
        "        assert emissions.shape[:2] == mask.shape\n",
        "        assert emissions.size(2) == self.num_tags\n",
        "        assert mask[0].all()\n",
        "\n",
        "        seq_length, batch_size = mask.shape\n",
        "\n",
        "        # Start transition and first emission\n",
        "        # shape: (batch_size, num_tags)\n",
        "        score = self.start_transitions + emissions[0]\n",
        "        history = []\n",
        "\n",
        "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
        "        # value at column j stores the score of the best tag sequence so far that ends\n",
        "        # with tag j\n",
        "        # history saves where the best tags candidate transitioned from; this is used\n",
        "        # when we trace back the best tag sequence\n",
        "\n",
        "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
        "        # for every possible next tag\n",
        "        for i in range(1, seq_length):\n",
        "            # Broadcast viterbi score for every possible next tag\n",
        "            # shape: (batch_size, num_tags, 1)\n",
        "            broadcast_score = score.unsqueeze(2)\n",
        "\n",
        "            # Broadcast emission score for every possible current tag\n",
        "            # shape: (batch_size, 1, num_tags)\n",
        "            broadcast_emission = emissions[i].unsqueeze(1)\n",
        "\n",
        "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
        "            # for each sample, entry at row i and column j stores the score of the best\n",
        "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
        "            # shape: (batch_size, num_tags, num_tags)\n",
        "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
        "\n",
        "            # Find the maximum score over all possible current tag\n",
        "            # shape: (batch_size, num_tags)\n",
        "            next_score, indices = next_score.max(dim=1)\n",
        "\n",
        "            # Set score to the next score if this timestep is valid (mask == 1)\n",
        "            # and save the index that produces the next score\n",
        "            # shape: (batch_size, num_tags)\n",
        "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
        "            history.append(indices)\n",
        "\n",
        "        history = torch.stack(history, dim=0)\n",
        "\n",
        "        # End transition score\n",
        "        # shape: (batch_size, num_tags)\n",
        "        score += self.end_transitions\n",
        "\n",
        "        # Now, compute the best path for each sample\n",
        "\n",
        "        # shape: (batch_size,)\n",
        "        seq_ends = mask.long().sum(dim=0) - 1\n",
        "        best_tags_list = []\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
        "            # for the last timestep\n",
        "            _, best_last_tag = score[idx].max(dim=0)\n",
        "            best_tags = [best_last_tag]\n",
        "\n",
        "            # We trace back where the best last tag comes from, append that to our best tag\n",
        "            # sequence, and trace it back again, and so on\n",
        "            for i, hist in enumerate(torch.flip(history[:seq_ends[idx]], dims=(0,))):\n",
        "                best_last_tag = hist[idx][best_tags[-1]]\n",
        "                best_tags.append(best_last_tag)\n",
        "\n",
        "            best_tags = torch.stack(best_tags, dim=0)\n",
        "\n",
        "            # Reverse the order because we start from the last timestep\n",
        "            best_tags_list.append(torch.flip(best_tags, dims=(0,)))\n",
        "\n",
        "        best_tags_list = nn.utils.rnn.pad_sequence(best_tags_list, batch_first=True, padding_value=0)\n",
        "\n",
        "        return best_tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KdozEy46dT4"
      },
      "source": [
        "### CRFLayer \n",
        "- Forward: decide output logits basaed on backbone network  \n",
        "- Decode: decode based on CRF weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNBE1vxOUJCe"
      },
      "outputs": [],
      "source": [
        "class CRFLayer(nn.Module):\n",
        "    def __init__(self, embedding_size, n_labels):\n",
        "\n",
        "        super(CRFLayer, self).__init__()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.output_dense = nn.Linear(embedding_size,n_labels)\n",
        "        self.crf = CRF(n_labels, batch_first=True)\n",
        "        self.token_from_subtoken = TokenFromSubtoken()\n",
        "\n",
        "    # Forward: decide output logits basaed on backbone network  \n",
        "    def forward(self, embedding, mask):\n",
        "        logits = self.output_dense(self.dropout(embedding))\n",
        "        logits = self.token_from_subtoken(logits, mask)\n",
        "        pad_mask = self.token_from_subtoken(mask.unsqueeze(-1), mask).squeeze(-1).bool()\n",
        "        return logits, pad_mask\n",
        "\n",
        "    # Decode: decode based on CRF weights \n",
        "    def decode(self, logits, pad_mask):\n",
        "        return self.crf.decode(logits, pad_mask)\n",
        "\n",
        "    # Evaluation Loss: calculate mean log likelihood of CRF layer\n",
        "    def eval_loss(self, logits, targets, pad_mask):\n",
        "        mean_log_likelihood = self.crf(logits, targets, pad_mask, reduction='sum').mean()\n",
        "        return -mean_log_likelihood\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ma5bWa7LaS"
      },
      "source": [
        "### NERModel\n",
        "- Roberta Model with CRF Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE7L4jPKTmPp"
      },
      "outputs": [],
      "source": [
        "class NERModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_labels:int, roberta_path:str):\n",
        "        super(NERModel,self).__init__()\n",
        "        self.roberta = XLMRobertaModel.from_pretrained(roberta_path)\n",
        "        self.crf = CRFLayer(self.roberta.config.hidden_size, n_labels)\n",
        "\n",
        "    # Forward: pass embedings to CRF layer in order to evaluate logits from suboword sequence\n",
        "    def forward(self, \n",
        "                input_ids:torch.Tensor,\n",
        "                attention_mask:torch.Tensor,\n",
        "                token_type_ids:torch.Tensor,\n",
        "                mask:torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        embedding = self.roberta(input_ids=input_ids,\n",
        "                                 attention_mask=attention_mask,\n",
        "                                 token_type_ids=token_type_ids)[0]\n",
        "        logits, pad_mask = self.crf(embedding, mask)\n",
        "        return logits, pad_mask\n",
        "\n",
        "    # Disable Gradient and Predict with model\n",
        "    @torch.no_grad()\n",
        "    def predict(self, inputs:Tuple[torch.Tensor]) -> torch.Tensor:\n",
        "        input_ids, attention_mask, token_type_ids, mask = inputs\n",
        "        logits, pad_mask = self(input_ids, attention_mask, token_type_ids, mask)\n",
        "        decoded = self.crf.decode(logits, pad_mask)\n",
        "        return decoded, pad_mask\n",
        "\n",
        "    # Decode: pass to crf decoder and decode based on CRF weights \n",
        "    def decode(self, logits, pad_mask):\n",
        "        \"\"\"Decode logits using CRF weights \n",
        "        \"\"\"\n",
        "        return self.crf.decode(logits, pad_mask) \n",
        "\n",
        "    # Evaluation Loss: pass to crf eval_loss and calculate mean log likelihood of CRF layer\n",
        "    def eval_loss(self, logits, targets, pad_mask):\n",
        "        return self.crf.eval_loss(logits, targets, pad_mask)\n",
        "\n",
        "    # Determine number of layers to be fine-tuned (!freeze) \n",
        "    def freeze_roberta(self, n_freeze:int=6):\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.roberta.encoder.layer[n_freeze:].parameters():\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBly2y20Kd_Q"
      },
      "source": [
        "### NERTokenizer\n",
        "- NLTK tokenizer along with XLMRobertaTokenizerFast tokenizer\n",
        "- Code adapted from the following [file](https://github.com/ugurcanozalp/multilingual-ner/blob/main/multiner/utils/custom_tokenizer.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtYHE7bFSg0X"
      },
      "outputs": [],
      "source": [
        "class NERTokenizer(object):\n",
        "\n",
        "    MAX_LEN=512\n",
        "    BATCH_LENGTH_LIMT = 380 # Max number of roberta tokens in one sentence.\n",
        "\n",
        "    # Modified version of http://stackoverflow.com/questions/36353125/nltk-regular-expression-tokenizer\n",
        "    PATTERN = r'''(?x)          # set flag to allow verbose regexps\n",
        "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A. or U.S.A # \n",
        "        | (?:\\d+\\.)           # numbers\n",
        "        | \\w+(?:[-.]\\w+)*     # words with optional internal hyphens\n",
        "        | \\$?\\d+(?:.\\d+)?%?   # currency and percentages, e.g. $12.40, 82%\n",
        "        | \\.\\.\\.              # ellipsis, and special chars below, includes ], [\n",
        "        | [-\\]\\[.؟،؛;\"'?,():_`“”/°º‘’″…#$%()*+<>=@\\\\^_{}|~❑&§\\!]\n",
        "        | \\u200c\n",
        "    '''\n",
        "\n",
        "    def __init__(self, base_model:str, to_device:str='cpu'):\n",
        "        super(NERTokenizer,self).__init__()\n",
        "        self.roberta_tokenizer = XLMRobertaTokenizerFast.from_pretrained(base_model, do_lower_case=False, padding=True, truncation=True)\n",
        "        self.to_device = to_device\n",
        "\n",
        "        self.word_tokenizer = RegexpTokenizer(self.PATTERN)\n",
        "        self.sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "    # tokenize batch of tokens\n",
        "    def tokenize_batch(self, inputs, pad_to = None) -> torch.Tensor:\n",
        "        batch = [inputs] if isinstance(inputs[0], str) else inputs\n",
        "        \n",
        "        input_ids, attention_mask, token_type_ids, mask = [], [], [], []\n",
        "        for tokens in batch:\n",
        "            input_ids_tmp, attention_mask_tmp, token_type_ids_tmp, mask_tmp = self._tokenize_words(tokens)\n",
        "            input_ids.append(input_ids_tmp)\n",
        "            attention_mask.append(attention_mask_tmp)\n",
        "            token_type_ids.append(token_type_ids_tmp)\n",
        "            mask.append(mask_tmp)\n",
        "\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.roberta_tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "        token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
        "        mask = pad_sequence(mask, batch_first=True, padding_value=0)\n",
        "        # truncate MAX_LEN\n",
        "        if input_ids.shape[-1]>self.MAX_LEN:\n",
        "            input_ids = input_ids[:,:,:self.MAX_LEN]\n",
        "            attention_mask = attention_mask[:,:,:self.MAX_LEN]\n",
        "            token_type_ids = token_type_ids[:,:,:self.MAX_LEN]\n",
        "            mask = mask[:,:,:self.MAX_LEN]\n",
        "        \n",
        "        # extend pad \n",
        "        elif pad_to is not None and pad_to>input_ids.shape[1]:\n",
        "            bs = input_ids.shape[0]\n",
        "            padlen = pad_to-input_ids.shape[1]\n",
        "\n",
        "            input_ids_append = torch.tensor([self.roberta_tokenizer.pad_token_id], dtype=torch.long).repeat([bs, padlen]).to(self.to_device)\n",
        "            input_ids = torch.cat([input_ids, input_ids_append], dim=-1)\n",
        "\n",
        "            attention_mask_append = torch.tensor([0], dtype=torch.long).repeat([bs, padlen]).to(self.to_device)\n",
        "            attention_mask = torch.cat([attention_mask, attention_mask_append], dim=-1)\n",
        "\n",
        "            token_type_ids_append = torch.tensor([0], dtype=torch.long).repeat([bs, padlen]).to(self.to_device)\n",
        "            token_type_ids = torch.cat([token_type_ids, token_type_ids_append], dim=-1)\n",
        "\n",
        "            mask_append = torch.tensor([0], dtype=torch.long).repeat([bs, padlen]).to(self.to_device)\n",
        "            mask = torch.cat([mask, mask_append], dim=-1)\n",
        "\n",
        "        # truncate pad\n",
        "        elif pad_to is not None and pad_to<input_ids.shape[1]:\n",
        "            input_ids = input_ids[:,:,:pad_to]\n",
        "            attention_mask = attention_mask[:,:,:pad_to]\n",
        "            token_type_ids = token_type_ids[:,:,:pad_to]\n",
        "            mask = mask[:,:,:pad_to]\n",
        "\n",
        "        if isinstance(inputs[0], str):\n",
        "            return input_ids[0], attention_mask[0], token_type_ids[0], mask[0]\n",
        "        else:\n",
        "            return input_ids, attention_mask, token_type_ids, mask\n",
        "\n",
        "    # tokenize list of words with roberta tokenizer\n",
        "    def _tokenize_words(self, words):\n",
        "        roberta_tokens = []\n",
        "        mask = []\n",
        "        for word in words:\n",
        "            subtokens = self.roberta_tokenizer.tokenize(word)\n",
        "            roberta_tokens+=subtokens\n",
        "            n_subtoken = len(subtokens)\n",
        "            if n_subtoken>=1:\n",
        "                mask = mask + [1] + [0]*(n_subtoken-1)\n",
        "\n",
        "        # add special tokens [CLS] and [SeP]\n",
        "        roberta_tokens = [self.roberta_tokenizer.cls_token] + roberta_tokens + [self.roberta_tokenizer.sep_token]\n",
        "        mask = [0] + mask + [0]\n",
        "        input_ids = torch.tensor(self.roberta_tokenizer.convert_tokens_to_ids(roberta_tokens), dtype=torch.long).to(self.to_device)\n",
        "        attention_mask = torch.ones(len(mask), dtype=torch.long).to(self.to_device)\n",
        "        token_type_ids = torch.zeros(len(mask), dtype=torch.long).to(self.to_device)\n",
        "        mask = torch.tensor(mask, dtype=torch.long).to(self.to_device)\n",
        "        return input_ids, attention_mask, token_type_ids, mask\n",
        "\n",
        "    # sent_to_token: yield each sentence token with positional span using nltk\n",
        "    def sent_to_token(self, raw_text):\n",
        "        for offset, ending in self.sent_tokenizer.span_tokenize(raw_text):\n",
        "            sub_text = raw_text[offset:ending]\n",
        "            words, spans = [], []\n",
        "            flush = False\n",
        "            total_subtoken = 0\n",
        "            for start, end in self.word_tokenizer.span_tokenize(sub_text):\n",
        "                flush = True\n",
        "                start += offset\n",
        "                end += offset\n",
        "                words.append(raw_text[start:end])\n",
        "                spans.append((start,end))\n",
        "                total_subtoken += len(self.roberta_tokenizer.tokenize(words[-1]))\n",
        "                if (total_subtoken > self.BATCH_LENGTH_LIMT): \n",
        "                    # Print\n",
        "                    yield words[:-1],spans[:-1]\n",
        "                    spans = spans[len(spans)-1:]\n",
        "                    words = words[len(words)-1:]\n",
        "                    total_subtoken = sum([len(self.roberta_tokenizer.tokenize(word)) for word in words])\n",
        "                    flush = False\n",
        "\n",
        "            if flush and len(spans) > 0:\n",
        "                yield words,spans\n",
        "\n",
        "    # Extract (batch words span() from a raw sentence\n",
        "    def prepare_row_text(self, raw_text, batch_size=16):\n",
        "        words_list, spans_list = [], []\n",
        "        end_batch = False\n",
        "        for words, spans in self.sent_to_token(raw_text):\n",
        "            end_batch = True\n",
        "            words_list.append(words)\n",
        "            spans_list.append(spans)\n",
        "            if len(spans_list) >= batch_size:\n",
        "                input_ids, attention_mask, token_type_ids, mask = self.tokenize_batch(words_list)\n",
        "                yield (input_ids, attention_mask, token_type_ids, mask), words_list, spans_list\n",
        "                words_list, spans_list = [], []\n",
        "        if end_batch and len(words_list) > 0:\n",
        "            input_ids, attention_mask, token_type_ids, mask = self.tokenize_batch(words_list)\n",
        "            yield (input_ids, attention_mask, token_type_ids, mask), words_list, spans_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxw6sJOpLShZ"
      },
      "source": [
        "### NERDataset\n",
        "- Pythorch Compatible Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM8jQXF3F28x"
      },
      "outputs": [],
      "source": [
        "# Pytorch Dataset\n",
        "class NERDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path:str, label_tags: List[str], base_model:str=\"xlm-roberta-base\", \n",
        "            default_label=0, max_length:int=512, to_device=\"cpu\"):\n",
        "        self.tokenizer = NERTokenizer(base_model=base_model, to_device=to_device)\n",
        "        self.label_tags = label_tags        \n",
        "        self.name_to_label = {x: i for i, x in enumerate(self.label_tags)}\n",
        "        self.default_label = default_label\n",
        "        self.max_length = max_length\n",
        "\n",
        "\n",
        "        # open file (train, test or val)\n",
        "        with open(data_path,'r') as f:\n",
        "            data_text = f.read()\n",
        "        self.data = []\n",
        "\n",
        "        # the loop notices the change of sentence with double newline \n",
        "        for sentence in filter(lambda x: len(x)>2, data_text.split('\\n\\n')):\n",
        "            sample = []\n",
        "            # each word laid in sepertaed lines\n",
        "            for wordline in sentence.split('\\n'):\n",
        "                if wordline=='':\n",
        "                    continue\n",
        "                # the word and label are seperated from each other with tab \n",
        "                word, label = wordline.split('\\t')\n",
        "                sample.append((word, label))\n",
        "            self.data.append(sample)\n",
        "\n",
        "    # len of dataset\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        words, labels = list(zip(*item))\n",
        "        \n",
        "        labels_idx = [self.name_to_label.get(x, self.default_label) for x in labels]  \n",
        "        y = torch.tensor(labels_idx, dtype=torch.long)\n",
        "        diff = self.max_length - y.shape[-1]\n",
        "        y = torch.nn.functional.pad(y, (0, diff), value=self.default_label)\n",
        "        X = self.tokenizer.tokenize_batch(list(words), pad_to=self.max_length)\n",
        "\n",
        "        return X, y "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PGD Adversarial Training\n",
        "- Code adapted from the following [file](https://github.com/lonePatient/BERT-NER-Pytorch/blob/master/callback/adversarial.py)\n",
        "- We chose this method becuase the NER task with adversarial training got the best F1 score and Accuracy overall in previous works, e.g. [link](https://github.com/lonePatient/BERT-NER-Pytorch/)."
      ],
      "metadata": {
        "id": "GXG5oIy92qwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD():\n",
        "    \"\"\"\n",
        "    https://towardsdatascience.com/know-your-enemy-7f7c5038bdf3\n",
        "    The PGD attack is a white-box attack which means the attacker has access to the model gradients i.e. the attacker \n",
        "    has a copy of your model’s weights. \"\"\"\n",
        "    def __init__(self, model,emb_name,epsilon=1.,alpha=0.3):\n",
        "        # The emb_name parameter should be replaced with the parameter name of the embedding in your model\n",
        "        self.model = model\n",
        "        self.emb_name = emb_name\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.emb_backup = {}\n",
        "        self.grad_backup = {}\n",
        "\n",
        "    # adversarial training : attack to change embedding abit with regards projected gradiant descent\n",
        "    def attack(self,first_strike=False):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and self.emb_name in name:\n",
        "                if first_strike:\n",
        "                    self.emb_backup[name] = param.data.clone()\n",
        "                norm = torch.norm(param.grad)\n",
        "                if norm != 0:\n",
        "                    # Compute new params\n",
        "                    r_at = self.alpha * param.grad / norm\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = self.project(name, param.data, self.epsilon)\n",
        "\n",
        "    # Restore to the back-up embeddings\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and self.emb_name in name:\n",
        "                assert name in self.emb_backup\n",
        "                param.data = self.emb_backup[name]\n",
        "        self.emb_backup = {}\n",
        "\n",
        "    # Project Gradiant Descent\n",
        "    def project(self, param_name, param_data, epsilon):\n",
        "        r = param_data - self.emb_backup[param_name]\n",
        "        if torch.norm(r) > epsilon:\n",
        "            r = epsilon * r / torch.norm(r)\n",
        "        return self.emb_backup[param_name] + r\n",
        "\n",
        "    # Back-up parameters\n",
        "    def backup_grad(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.grad_backup[name] = param.grad.clone()\n",
        "\n",
        "    # Restore grad parameters\n",
        "    def restore_grad(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.grad = self.grad_backup[name]"
      ],
      "metadata": {
        "id": "wF45YtYZ2uIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrgxfjlM2qw"
      },
      "source": [
        "### NERWrapper\n",
        "- Lightining Wrapper for NER Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLQUzO8qGRA-"
      },
      "outputs": [],
      "source": [
        "# Lightining Wrapper for NER Model\n",
        "\n",
        "class NERWrapper(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "        learning_rate = 2e-5,\n",
        "        weight_decay = 0.0,\n",
        "        batch_size = 16,\n",
        "        freeze_layers = 8,\n",
        "        tags = TAGS_TABLE,\n",
        "        train_path = \"./data/train.txt\",\n",
        "        val_path = \"./data/val.txt\",\n",
        "        test_path =\"./data/test.txt\",\n",
        "        pretrained_path = None,\n",
        "        do_adv = False,\n",
        "        *args, **kwargs\n",
        "    ):\n",
        "        \n",
        "        super(NERWrapper,self).__init__()\n",
        "        self.save_hyperparameters('learning_rate', 'weight_decay', 'batch_size')\n",
        "        self.tags, self.train_path, self.val_path, self.test_path = tags, train_path, val_path, test_path\n",
        "        self.model = NERModel(n_labels=len(self.tags), roberta_path=\"xlm-roberta-base\")\n",
        "        self.do_adv = do_adv\n",
        "\n",
        "        # Load Pre-trained data if available\n",
        "        if pretrained_path is not None:\n",
        "            self.model.load_state_dict(torch.load(pretrained_path))\n",
        "        self.model.freeze_roberta(freeze_layers)\n",
        "\n",
        "        # Instanciate PGD class for adversarial training\n",
        "        self.pgd = PGD(\n",
        "            model=self.model,\n",
        "            emb_name=\"word_embeddings.\"\n",
        "        )\n",
        "        # Set automatic optimazation false becuase we need to inject adversarial training to regular PyTorch Training\n",
        "        # mechanism\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    # Typical PyTorch Lightening Forward method(Override)\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.model.forward(*args, **kwargs)\n",
        "\n",
        "    # Intersection of Step process for all actions: train/val/test\n",
        "    # Predict labels -> Calculate Loss -> Update parameters and Log changes\n",
        "    def _step(self, batch, batch_idx):\n",
        "        (input_ids, attention_mask, token_type_ids, mask), labels = batch\n",
        "        logits, pad_mask = self.model(input_ids, attention_mask, token_type_ids, mask)\n",
        "        labels = labels[:, :logits.shape[1]]\n",
        "        loss = self.model.eval_loss(logits, labels, pad_mask)\n",
        "        preds_tag_idx = self.model.decode(logits, pad_mask)\n",
        "        preds_tag = [[self.tags[start.item()] for m, start in zip(mask, sample) if m] for mask, sample in zip(pad_mask, preds_tag_idx)]\n",
        "        labels_tag = [[self.tags[start.item()] for m, start in zip(mask, sample) if m] for mask, sample in zip(pad_mask, labels)]\n",
        "        tensorboard_logs = {'batch_loss': loss}\n",
        "        for metric, value in tensorboard_logs.items():\n",
        "            self.log(metric, value, prog_bar=True)\n",
        "        return {'loss': loss, \"preds\": preds_tag, \"labels\": labels_tag}\n",
        "\n",
        "    # Customized Training Step (Not anymore Typical :)\n",
        "    # Support both Adversarial / Normal training\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Get Optimizer\n",
        "        opt = self.optimizers(use_pl_optimizer=True)\n",
        "        # Calculate normal loss and backward\n",
        "        loss = self._step(batch, batch_idx)['loss']\n",
        "        self.manual_backward(loss)\n",
        "\n",
        "        # Check if Adversarial Training is enabled\n",
        "        if self.do_adv:\n",
        "            # PGD Start\n",
        "            self.pgd.backup_grad()\n",
        "            attack_times = 3\n",
        "            (input_ids, attention_mask, token_type_ids, mask), labels = batch\n",
        "            for attack_time in range(attack_times):\n",
        "                # Add adversarial perturbation to the embedding, backup param.data during the first attack\n",
        "                self.pgd.attack(first_strike=(attack_time==0))\n",
        "                if attack_time != attack_times-1:\n",
        "                    self.model.zero_grad()\n",
        "                else:\n",
        "                    self.pgd.restore_grad()\n",
        "\n",
        "                logits_adv, pad_mask_adv = self.model(input_ids, attention_mask, token_type_ids, mask)\n",
        "                labels_adv = labels[:, :logits_adv.shape[1]]\n",
        "                loss_adv = self.model.eval_loss(logits_adv, labels_adv, pad_mask_adv)\n",
        "                \n",
        "                # Backpropagation, and on the basis of the normal grad, accumulate the gradient of the adversarial training\n",
        "                self.manual_backward(loss_adv) \n",
        "\n",
        "            # Restore embedding parameters\n",
        "            self.pgd.restore() \n",
        "\n",
        "        opt.step()\n",
        "        # Reset gradiants\n",
        "        opt.zero_grad()\n",
        "\n",
        "    # Typical PyTorch Validation Step\n",
        "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        return self._step(batch, batch_idx)\n",
        "\n",
        "    # Typical PyTorch Test Step\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        return self._step(batch, batch_idx)\n",
        "\n",
        "    # Having different learning rate for \"bias\", \"LayerNorm.weight\" is Awsome Idea which we learnt from the following\n",
        "    # link https://huggingface.co/transformers/v3.3.1/training.html\n",
        "    def configure_optimizers(self):\n",
        "        no_decay_keywords = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n,p in self.model.named_parameters() if not any(nd in n for nd in no_decay_keywords)],\n",
        "                \"weight_decay_rate\": self.hparams.weight_decay,\n",
        "                \"lr\": self.hparams.learning_rate,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n,p in self.model.named_parameters() if any(nd in n for nd in no_decay_keywords)],\n",
        "                \"weight_decay_rate\": 0,\n",
        "                \"lr\": self.hparams.learning_rate,\n",
        "            }\n",
        "        ]\n",
        "        # AdamW Optimizer with grouped parameters\n",
        "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
        "        return optimizer\n",
        "\n",
        "    # Use PyTorch NER Dataset Loader\n",
        "    def _dataloader(self, path, shuffle=False):\n",
        "        dataset = NERDataset(path, label_tags = self.tags, default_label=0, to_device=self.device)\n",
        "        return DataLoader(dataset, drop_last=False, shuffle=shuffle, batch_size=self.hparams.batch_size, \n",
        "            worker_init_fn=np.random.seed(0))\n",
        "\n",
        "    # Note: Shuffle = Flase\n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_path, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_path)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self._dataloader(self.test_path)\n",
        "\n",
        "\n",
        "    # Log all metrics at the end of each epoch\n",
        "    def _epoch_end (self, outputs):\n",
        "        preds = sum([x['preds'] for x in outputs], [])\n",
        "        labels = sum([x['labels'] for x in outputs], [])\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        precision = precision_score(labels, preds, mode='strict', scheme=IOB2, average='micro', zero_division=1)\n",
        "        recall = recall_score(labels, preds, mode='strict', scheme=IOB2, average='micro', zero_division=1)\n",
        "        f1 = f1_score(labels, preds, mode='strict', scheme=IOB2, average='micro', zero_division=1)\n",
        "        tensorboard_logs = {'loss': loss, 'acc': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "        return tensorboard_logs\n",
        "\n",
        "    # Log all Evaluation metrics at the end of each epoch\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        val_logs = self._epoch_end(outputs)\n",
        "        tensorboard_logs = {'val_loss': val_logs['loss'], 'val_accuracy': val_logs['acc'], 'val_precision': val_logs['precision'], 'val_recall': val_logs['recall'], 'val_F1': val_logs['f1']}\n",
        "        for metric, value in tensorboard_logs.items():\n",
        "            self.log(metric, value, prog_bar=True)\n",
        " \n",
        "    # Log all Test metrics at the end of each epoch\n",
        "    def test_epoch_end(self, outputs):\n",
        "        test_logs = self._epoch_end(outputs)\n",
        "        tensorboard_logs = {'test_loss': test_logs['loss'], 'test_accuracy': test_logs['acc'], 'test_precision': test_logs['precision'], 'test_recall': test_logs['recall'], 'test_F1': test_logs['f1']}\n",
        "        for metric, value in tensorboard_logs.items():\n",
        "            self.log(metric, value, prog_bar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train With Strongly labeled data"
      ],
      "metadata": {
        "id": "n9X8dNJKQMat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "checkpoint_callback = ModelCheckpoint(save_weights_only=True)\n",
        "\n",
        "plmodel = NERWrapper(pretrained_path='./model/HengamTransW.pth', \n",
        "                     train_path = './strong/strong_train.txt',\n",
        "                     val_path = './strong/strong_val.txt',\n",
        "                     test_path='./gold/gold.txt',\n",
        "                     do_adv = True,\n",
        "                     freeze_layers=8)\n",
        "\n",
        "trainer = pl.Trainer(callbacks=[checkpoint_callback], gpus=1, max_epochs=2, log_every_n_steps=1)"
      ],
      "metadata": {
        "id": "4K9vQoa_AX1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(plmodel)"
      ],
      "metadata": {
        "id": "mDqNZhbmCA5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(plmodel.model.state_dict(), os.path.join(MODEL_PATH, 'HengamTransA.pth'))"
      ],
      "metadata": {
        "id": "-qIqFBX8Qgem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# strict evaluation\n",
        "plmodel.eval()\n",
        "result = trainer.test(plmodel)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "dsFQFnikEzr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}